{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":93787,"databundleVersionId":11165379,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"63011854","cell_type":"markdown","source":"# ðŸ§  Histopathology OOD Classification - DINOv2 + Scenario 2 Inspired Augmentations","metadata":{}},{"id":"e1f47bd1","cell_type":"code","source":"import h5py\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.transforms import InterpolationMode\nimport torch.nn as nn\nimport random\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import balanced_accuracy_score\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Chemins\nTRAIN_IMAGES_PATH = '/kaggle/input/mva-dlmi-2025-histopathology-ood-classification/train.h5'\nVAL_IMAGES_PATH = '/kaggle/input/mva-dlmi-2025-histopathology-ood-classification/val.h5'\nTEST_IMAGES_PATH = '/kaggle/input/mva-dlmi-2025-histopathology-ood-classification/test.h5'\n\nSEED = 42\nrandom.seed(SEED)\ntorch.manual_seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T12:01:09.914341Z","iopub.execute_input":"2025-04-11T12:01:09.914765Z","iopub.status.idle":"2025-04-11T12:01:21.127863Z","shell.execute_reply.started":"2025-04-11T12:01:09.914731Z","shell.execute_reply":"2025-04-11T12:01:21.126863Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7ae832b49c30>"},"metadata":{}}],"execution_count":1},{"id":"5239febe-84aa-4b24-aa3c-9c60662a229b","cell_type":"code","source":"transform_train = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomApply([\n        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n        transforms.GaussianBlur(kernel_size=3)\n    ], p=0.5),\n    transforms.Resize((98, 98), interpolation=InterpolationMode.BICUBIC),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)\n])\n\ntransform_val = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((98, 98), interpolation=InterpolationMode.BICUBIC),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)\n])","metadata":{"execution":{"iopub.status.busy":"2025-04-11T12:01:21.129148Z","iopub.execute_input":"2025-04-11T12:01:21.129588Z","iopub.status.idle":"2025-04-11T12:01:21.136172Z","shell.execute_reply.started":"2025-04-11T12:01:21.129563Z","shell.execute_reply":"2025-04-11T12:01:21.135027Z"},"trusted":true},"outputs":[],"execution_count":2},{"id":"9a822421-d623-4dda-b3ac-80bc5dd7c99b","cell_type":"code","source":"class BaselineDataset(Dataset):\n    def __init__(self, dataset_path, transform, mode):\n        self.dataset_path = dataset_path\n        self.transform = transform\n        self.mode = mode\n        with h5py.File(self.dataset_path, 'r') as hdf:\n            self.ids = list(hdf.keys())\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, idx):\n        with h5py.File(self.dataset_path, 'r') as hdf:\n            img_id = self.ids[idx]\n            img = np.array(hdf[img_id]['img'], dtype=np.float32)\n            label = np.array(hdf[img_id]['label']) if self.mode == 'train' else None\n        if img.shape[0] != 3:  # (H, W, C) â†’ (C, H, W)\n            img = torch.tensor(img).permute(2, 0, 1)\n        else:\n            img = torch.tensor(img) \n        img = self.transform(img)\n        return img, label","metadata":{"execution":{"iopub.status.busy":"2025-04-11T12:01:21.138206Z","iopub.execute_input":"2025-04-11T12:01:21.138528Z","iopub.status.idle":"2025-04-11T12:01:21.149856Z","shell.execute_reply.started":"2025-04-11T12:01:21.138492Z","shell.execute_reply":"2025-04-11T12:01:21.148927Z"},"trusted":true},"outputs":[],"execution_count":3},{"id":"5f35d39a-d72e-47df-a325-7414705e4391","cell_type":"code","source":"class DoubleInputDataset(Dataset):\n    def __init__(self, dataset_path, transform, mode):\n        self.dataset_path = dataset_path\n        self.transform = transform\n        self.mode = mode\n        with h5py.File(self.dataset_path, 'r') as hdf:\n            self.ids = list(hdf.keys())\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, idx):\n        with h5py.File(self.dataset_path, 'r') as hdf:\n            img_id = self.ids[idx]\n            img = np.array(hdf[img_id]['img'], dtype=np.float32)\n            label = np.array(hdf[img_id]['label']) if self.mode == 'train' else None\n        \n        if img.shape[0] != 3:\n            img = torch.tensor(img).permute(2, 0, 1)\n        else:\n            img = torch.tensor(img)\n\n        # Random crop for local patch\n        local_patch = transforms.RandomCrop((64, 64))(img)\n        # Resize entire image for context\n        context_patch = transforms.Resize((98, 98), interpolation=InterpolationMode.BICUBIC)(img)\n\n        local_patch = self.transform(local_patch)\n        context_patch = self.transform(context_patch)\n\n        return (local_patch, context_patch), label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T12:01:21.151435Z","iopub.execute_input":"2025-04-11T12:01:21.151718Z","iopub.status.idle":"2025-04-11T12:01:21.168865Z","shell.execute_reply.started":"2025-04-11T12:01:21.151692Z","shell.execute_reply":"2025-04-11T12:01:21.167947Z"}},"outputs":[],"execution_count":4},{"id":"d7264b9f-814b-4439-b519-61172ca1d89d","cell_type":"code","source":"BATCH_SIZE = 32\n# train_dataset = BaselineDataset(TRAIN_IMAGES_PATH, transform_train, 'train')\n# val_dataset = BaselineDataset(VAL_IMAGES_PATH, transform_val, 'train')\n\ntrain_dataset = DoubleInputDataset(TRAIN_IMAGES_PATH, transform_train, mode='train')\nval_dataset = DoubleInputDataset(VAL_IMAGES_PATH, transform_val, 'train')\n\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True)\nval_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2025-04-11T12:01:21.169889Z","iopub.execute_input":"2025-04-11T12:01:21.170272Z","iopub.status.idle":"2025-04-11T12:02:41.213770Z","shell.execute_reply.started":"2025-04-11T12:01:21.170236Z","shell.execute_reply":"2025-04-11T12:02:41.212758Z"},"trusted":true},"outputs":[],"execution_count":5},{"id":"2db2edfb-20b4-405f-8722-370518645981","cell_type":"code","source":"feature_extractor = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(device)\nfeature_extractor.eval()","metadata":{"execution":{"iopub.status.busy":"2025-04-11T12:02:41.214628Z","iopub.execute_input":"2025-04-11T12:02:41.214897Z","iopub.status.idle":"2025-04-11T12:02:46.064569Z","shell.execute_reply.started":"2025-04-11T12:02:41.214875Z","shell.execute_reply":"2025-04-11T12:02:46.063692Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /root/.cache/torch/hub/main.zip\n/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n  warnings.warn(\"xFormers is not available (SwiGLU)\")\n/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n  warnings.warn(\"xFormers is not available (Attention)\")\n/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n  warnings.warn(\"xFormers is not available (Block)\")\nDownloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vits14_pretrain.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84.2M/84.2M [00:01<00:00, 55.6MB/s]\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DinoVisionTransformer(\n  (patch_embed): PatchEmbed(\n    (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n    (norm): Identity()\n  )\n  (blocks): ModuleList(\n    (0-11): 12 x NestedTensorBlock(\n      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (attn): MemEffAttention(\n        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): LayerScale()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n        (act): GELU(approximate='none')\n        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n        (drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): LayerScale()\n      (drop_path2): Identity()\n    )\n  )\n  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n  (head): Identity()\n)"},"metadata":{}}],"execution_count":6},{"id":"b55bb527-3efd-4cb8-856f-b492efd62a4d","cell_type":"code","source":"def precompute(dataloader, model, device):\n    xs, ys = [], []\n    for x, y in tqdm(dataloader):\n        with torch.no_grad():\n            feats = model(x.to(device)).detach().cpu()\n        xs.append(feats)\n        ys.append(torch.tensor(y))\n    return torch.cat(xs), torch.cat(ys)\n\ndef precompute_double(dataloader, model, device):\n    xs, ys = [], []\n    for (x_local, x_context), y in tqdm(dataloader):\n        with torch.no_grad():\n            feats_local = model(x_local.to(device)).detach().cpu()\n            feats_context = model(x_context.to(device)).detach().cpu()\n            feats = torch.cat([feats_local, feats_context], dim=1)\n        xs.append(feats)\n        ys.append(torch.tensor(y))\n    return torch.cat(xs), torch.cat(ys)\n\nx_train, y_train = precompute_double(train_dataloader, feature_extractor, device)\nx_val, y_val = precompute_double(val_dataloader, feature_extractor, device)","metadata":{"execution":{"iopub.status.busy":"2025-04-11T12:02:46.065553Z","iopub.execute_input":"2025-04-11T12:02:46.065816Z","iopub.status.idle":"2025-04-11T13:55:39.516363Z","shell.execute_reply.started":"2025-04-11T12:02:46.065793Z","shell.execute_reply":"2025-04-11T13:55:39.515162Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccf83295d154462f87606fe93a479542"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-7-0f47a3d9f700>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  ys.append(torch.tensor(y))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1091 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2126539c49c645b486d5d8ee82c3dd5e"}},"metadata":{}}],"execution_count":7},{"id":"8740b0ee-ffc7-40dc-9d80-a6537785e458","cell_type":"code","source":"class PrecomputedDataset(Dataset):\n    def __init__(self, features, labels):\n        self.features = features\n        self.labels = labels.unsqueeze(1).float()\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return self.features[idx], self.labels[idx]\n\ntrain_ds = PrecomputedDataset(x_train, y_train)\nval_ds = PrecomputedDataset(x_val, y_val)\ntrain_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2025-04-11T13:55:39.519944Z","iopub.execute_input":"2025-04-11T13:55:39.520346Z","iopub.status.idle":"2025-04-11T13:55:39.528385Z","shell.execute_reply.started":"2025-04-11T13:55:39.520310Z","shell.execute_reply":"2025-04-11T13:55:39.527444Z"},"trusted":true},"outputs":[],"execution_count":8},{"id":"ab778b23-550c-4abf-ac5c-dd0cce9abb4f","cell_type":"code","source":"class SimpleLinearProbeSmall(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(input_dim, 32),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(32, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\nmodel = SimpleLinearProbeSmall(x_train.shape[1]).to(device)","metadata":{"execution":{"iopub.status.busy":"2025-04-11T13:55:39.529832Z","iopub.execute_input":"2025-04-11T13:55:39.530246Z","iopub.status.idle":"2025-04-11T13:55:39.553249Z","shell.execute_reply.started":"2025-04-11T13:55:39.530206Z","shell.execute_reply":"2025-04-11T13:55:39.552317Z"},"trusted":true},"outputs":[],"execution_count":9},{"id":"6cef2e55-321d-4eb0-9419-b9ae614e4760","cell_type":"code","source":"import torch.optim as optim\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\ncriterion = nn.BCELoss()\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # Reduce LR every 10 epochs by a factor of 0.1\n\nbest_loss = float('inf')\nbest_acc = float('-inf')\nbest_epoch = 0\nPATIENCE = 10\nNUM_EPOCHS = 50\n\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    losses, preds, targets = [], [], []\n    for xb, yb in train_dl:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        output = model(xb)\n        loss = criterion(output, yb)\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n        preds.extend(output.detach().cpu().numpy())\n        targets.extend(yb.cpu().numpy())\n    acc = balanced_accuracy_score(targets, np.array(preds) > 0.5)\n    print(f\"[Train] Epoch {epoch+1} - Loss: {np.mean(losses):.4f} - BalAcc: {acc:.4f}\")\n\n    # Validation\n    model.eval()\n    losses, preds, targets = [], [], []\n    with torch.no_grad():\n        for xb, yb in val_dl:\n            xb, yb = xb.to(device), yb.to(device)\n            output = model(xb)\n            loss = criterion(output, yb)\n            losses.append(loss.item())\n            preds.extend(output.cpu().numpy())\n            targets.extend(yb.cpu().numpy())\n    val_acc = balanced_accuracy_score(targets, np.array(preds) > 0.5)\n    val_loss = np.mean(losses)\n    print(f\"[Val]   Epoch {epoch+1} - Loss: {val_loss:.4f} - BalAcc: {val_acc:.4f}\")\n\n    # if val_loss < best_loss:\n    #     best_loss = val_loss\n    #     best_epoch = epoch\n    #     torch.save(model.state_dict(), 'best_model.pth')\n\n    if val_acc > best_acc:\n        best_acc = val_acc\n        best_epoch = epoch\n        torch.save(model.state_dict(), 'best_model.pth')\n\n    if epoch - best_epoch >= PATIENCE:\n        print(\"Early stopping.\")\n        break\n    scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2025-04-11T13:55:39.554371Z","iopub.execute_input":"2025-04-11T13:55:39.554772Z","iopub.status.idle":"2025-04-11T13:58:08.005511Z","shell.execute_reply.started":"2025-04-11T13:55:39.554733Z","shell.execute_reply":"2025-04-11T13:58:08.004521Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[Train] Epoch 1 - Loss: 0.1440 - BalAcc: 0.9455\n[Val]   Epoch 1 - Loss: 0.3075 - BalAcc: 0.8714\n[Train] Epoch 2 - Loss: 0.1151 - BalAcc: 0.9572\n[Val]   Epoch 2 - Loss: 0.2434 - BalAcc: 0.9027\n[Train] Epoch 3 - Loss: 0.1057 - BalAcc: 0.9604\n[Val]   Epoch 3 - Loss: 0.2828 - BalAcc: 0.8961\n[Train] Epoch 4 - Loss: 0.0999 - BalAcc: 0.9628\n[Val]   Epoch 4 - Loss: 0.2660 - BalAcc: 0.9042\n[Train] Epoch 5 - Loss: 0.0963 - BalAcc: 0.9641\n[Val]   Epoch 5 - Loss: 0.3260 - BalAcc: 0.8840\n[Train] Epoch 6 - Loss: 0.0945 - BalAcc: 0.9646\n[Val]   Epoch 6 - Loss: 0.2646 - BalAcc: 0.8962\n[Train] Epoch 7 - Loss: 0.0913 - BalAcc: 0.9656\n[Val]   Epoch 7 - Loss: 0.2579 - BalAcc: 0.8953\n[Train] Epoch 8 - Loss: 0.0899 - BalAcc: 0.9662\n[Val]   Epoch 8 - Loss: 0.2389 - BalAcc: 0.9052\n[Train] Epoch 9 - Loss: 0.0884 - BalAcc: 0.9669\n[Val]   Epoch 9 - Loss: 0.2698 - BalAcc: 0.8991\n[Train] Epoch 10 - Loss: 0.0875 - BalAcc: 0.9676\n[Val]   Epoch 10 - Loss: 0.2759 - BalAcc: 0.9022\n[Train] Epoch 11 - Loss: 0.0729 - BalAcc: 0.9730\n[Val]   Epoch 11 - Loss: 0.2675 - BalAcc: 0.9030\n[Train] Epoch 12 - Loss: 0.0695 - BalAcc: 0.9742\n[Val]   Epoch 12 - Loss: 0.2721 - BalAcc: 0.9048\n[Train] Epoch 13 - Loss: 0.0681 - BalAcc: 0.9745\n[Val]   Epoch 13 - Loss: 0.2733 - BalAcc: 0.9030\n[Train] Epoch 14 - Loss: 0.0676 - BalAcc: 0.9746\n[Val]   Epoch 14 - Loss: 0.2844 - BalAcc: 0.9030\n[Train] Epoch 15 - Loss: 0.0669 - BalAcc: 0.9748\n[Val]   Epoch 15 - Loss: 0.2858 - BalAcc: 0.9013\n[Train] Epoch 16 - Loss: 0.0657 - BalAcc: 0.9752\n[Val]   Epoch 16 - Loss: 0.2889 - BalAcc: 0.8997\n[Train] Epoch 17 - Loss: 0.0644 - BalAcc: 0.9756\n[Val]   Epoch 17 - Loss: 0.2961 - BalAcc: 0.9008\n[Train] Epoch 18 - Loss: 0.0637 - BalAcc: 0.9767\n[Val]   Epoch 18 - Loss: 0.3057 - BalAcc: 0.9011\nEarly stopping.\n","output_type":"stream"}],"execution_count":10},{"id":"14f91749-8f3e-4fe8-9eb3-b750b6d17456","cell_type":"code","source":"'''model.load_state_dict(torch.load('best_model.pth'))\nmodel.eval()\n\nsubmission = {'ID': [], 'Pred': []}\nwith h5py.File(TEST_IMAGES_PATH, 'r') as hdf:\n    for img_id in tqdm(hdf.keys()):\n        img = torch.tensor(np.array(hdf[img_id]['img'], dtype=np.float32))  # plus de permute\n        img = transform_val(img).unsqueeze(0).to(device)\n        with torch.no_grad():\n            pred = model(feature_extractor(img)).item()\n        submission['ID'].append(int(img_id))\n        submission['Pred'].append(int(pred > 0.5))\n\nsubmission_df = pd.DataFrame(submission).set_index('ID')\nsubmission_df.to_csv(\"submission.csv\")\nprint(\"âœ… Fichier submission.csv gÃ©nÃ©rÃ©.\") '''","metadata":{"execution":{"iopub.status.busy":"2025-04-11T13:58:08.006785Z","iopub.execute_input":"2025-04-11T13:58:08.007172Z","iopub.status.idle":"2025-04-11T13:58:08.014267Z","shell.execute_reply.started":"2025-04-11T13:58:08.007134Z","shell.execute_reply":"2025-04-11T13:58:08.013245Z"},"trusted":true},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'model.load_state_dict(torch.load(\\'best_model.pth\\'))\\nmodel.eval()\\n\\nsubmission = {\\'ID\\': [], \\'Pred\\': []}\\nwith h5py.File(TEST_IMAGES_PATH, \\'r\\') as hdf:\\n    for img_id in tqdm(hdf.keys()):\\n        img = torch.tensor(np.array(hdf[img_id][\\'img\\'], dtype=np.float32))  # plus de permute\\n        img = transform_val(img).unsqueeze(0).to(device)\\n        with torch.no_grad():\\n            pred = model(feature_extractor(img)).item()\\n        submission[\\'ID\\'].append(int(img_id))\\n        submission[\\'Pred\\'].append(int(pred > 0.5))\\n\\nsubmission_df = pd.DataFrame(submission).set_index(\\'ID\\')\\nsubmission_df.to_csv(\"submission.csv\")\\nprint(\"âœ… Fichier submission.csv gÃ©nÃ©rÃ©.\") '"},"metadata":{}}],"execution_count":11},{"id":"3f6d6be6","cell_type":"markdown","source":"## ðŸ§ª Test-Time Augmentation (TTA)","metadata":{}},{"id":"cb0795be","cell_type":"code","source":"tta_transforms = [\n    transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((98, 98), interpolation=InterpolationMode.BICUBIC),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5]*3, [0.5]*3)\n    ]),\n    transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((98, 98), interpolation=InterpolationMode.BICUBIC),\n        transforms.RandomHorizontalFlip(p=1.0),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5]*3, [0.5]*3)\n    ]),\n    transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((98, 98), interpolation=InterpolationMode.BICUBIC),\n        transforms.RandomVerticalFlip(p=1.0),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5]*3, [0.5]*3)\n    ]),\n    transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((98, 98), interpolation=InterpolationMode.BICUBIC),\n        transforms.RandomRotation(15),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5]*3, [0.5]*3)\n    ])\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:58:08.015462Z","iopub.execute_input":"2025-04-11T13:58:08.015819Z","iopub.status.idle":"2025-04-11T13:58:08.040068Z","shell.execute_reply.started":"2025-04-11T13:58:08.015783Z","shell.execute_reply":"2025-04-11T13:58:08.038946Z"}},"outputs":[],"execution_count":12},{"id":"19607341-c423-4906-bcab-6527bf07bc83","cell_type":"code","source":"def precompute_double_tta(dataloader, model, device, tta_transforms):\n    xs, ys = [], []\n    for (x_local, x_context), y in tqdm(dataloader):\n        local_feats, context_feats = [], []\n        for tta_transform in tta_transforms:\n            x_local_tta = tta_transform(x_local)\n            x_context_tta = tta_transform(x_context)\n            with torch.no_grad():\n                f_local = model(x_local_tta.to(device)).detach().cpu()\n                f_context = model(x_context_tta.to(device)).detach().cpu()\n            local_feats.append(f_local)\n            context_feats.append(f_context)\n\n        # Moyenne sur les TTA\n        f_local = torch.stack(local_feats).mean(dim=0)\n        f_context = torch.stack(context_feats).mean(dim=0)\n        f = torch.cat([f_local, f_context], dim=1)\n        xs.append(f)\n        ys.append(torch.tensor(y))\n\n    return torch.cat(xs), torch.cat(ys)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T17:29:00.104675Z","iopub.execute_input":"2025-04-10T17:29:00.105145Z","iopub.status.idle":"2025-04-10T17:29:00.124042Z","shell.execute_reply.started":"2025-04-10T17:29:00.105061Z","shell.execute_reply":"2025-04-10T17:29:00.122572Z"}},"outputs":[],"execution_count":13},{"id":"bb314427-56ea-439e-a86f-47b6720663a6","cell_type":"code","source":"# model.load_state_dict(torch.load('best_model.pth'))\n# model.eval()\n\n# submission = {'ID': [], 'Pred': []}\n# with h5py.File(TEST_IMAGES_PATH, 'r') as hdf:\n#     for img_id in tqdm(hdf.keys()):\n#         img_raw = np.array(hdf[img_id]['img'], dtype=np.float32)\n#         if img_raw.shape[0] != 3:\n#             img_raw = torch.tensor(img_raw).permute(2, 0, 1)\n#         else:\n#             img_raw = torch.tensor(img_raw)\n\n#         tta_imgs = torch.stack([t(img_raw) for t in tta_transforms]).to(device)\n\n#         with torch.no_grad():\n#             features = feature_extractor(tta_imgs)\n#             output = model(features).squeeze(1)  # shape: (N_TTA,)\n#             pred = output.mean().item()\n\n#         submission['ID'].append(int(img_id))\n#         submission['Pred'].append(int(pred > 0.5))\n\n\n# submission_df = pd.DataFrame(submission).set_index('ID')\n# submission_df.to_csv(\"submission_doubleinput.csv\")\n# print(\"âœ… Fichier submission.csv gÃ©nÃ©rÃ© avec TTA.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:58:08.041045Z","iopub.execute_input":"2025-04-11T13:58:08.041410Z","iopub.status.idle":"2025-04-11T13:58:08.050117Z","shell.execute_reply.started":"2025-04-11T13:58:08.041375Z","shell.execute_reply":"2025-04-11T13:58:08.049044Z"}},"outputs":[],"execution_count":13},{"id":"58fdcb94-e48a-4ffa-a23c-e52fd7123f7c","cell_type":"code","source":"model.load_state_dict(torch.load('best_model.pth'))\nmodel.eval()\n\nsubmission = {'ID': [], 'Pred': []}\nwith h5py.File(TEST_IMAGES_PATH, 'r') as hdf:\n    for img_id in tqdm(hdf.keys()):\n        img_raw = np.array(hdf[img_id]['img'], dtype=np.float32)\n        if img_raw.shape[0] != 3:\n            img_raw = torch.tensor(img_raw).permute(2, 0, 1)\n        else:\n            img_raw = torch.tensor(img_raw)\n\n        # CrÃ©er local + context\n        local_patch = transforms.CenterCrop((64, 64))(img_raw)\n        context_patch = transforms.Resize((98, 98), interpolation=InterpolationMode.BICUBIC)(img_raw)\n\n        # Appliquer TTA Ã  chacun (en parallÃ¨le)\n        local_ttas = torch.stack([t(local_patch) for t in tta_transforms]).to(device)\n        context_ttas = torch.stack([t(context_patch) for t in tta_transforms]).to(device)\n\n        with torch.no_grad():\n            feat_local = feature_extractor(local_ttas)\n            feat_context = feature_extractor(context_ttas)\n\n        # Moyenne des features\n        feat_local_mean = feat_local.mean(dim=0)\n        feat_context_mean = feat_context.mean(dim=0)\n\n        features = torch.cat([feat_local_mean, feat_context_mean]).unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            pred = model(features).item()\n\n        submission['ID'].append(int(img_id))\n        submission['Pred'].append(int(pred > 0.5))  # seuil Ã  0.5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:58:08.051245Z","iopub.execute_input":"2025-04-11T13:58:08.051598Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-14-c99ccf684dec>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pth'))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/85054 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ce5e7a40cb54f45a48ad557cd3c7039"}},"metadata":{}}],"execution_count":null},{"id":"dd588b82-0581-44f6-b252-44fd085cb3e6","cell_type":"code","source":"sumbission_df.to_csv(\"sub_TTA.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"19bbde72-b8f7-465b-959a-551bcc52d4e1","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}